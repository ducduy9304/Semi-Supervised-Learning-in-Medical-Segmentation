{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687b5d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import h5py\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.ndimage import zoom, rotate\n",
    "import itertools\n",
    "from torch.utils.data import Sampler\n",
    "from medpy import metric\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorboardX import SummaryWriter\n",
    "from skimage.measure import label \n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aa3b66",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac90409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class params: \n",
    "    def __init__(self): \n",
    "        self.root_dir = 'ACDC' \n",
    "        self.exp = 'MBCP' \n",
    "        self.model = 'unet' \n",
    "        self.pretrain_iterations = 50\n",
    "        \n",
    "        self.selftrain_iterations = 10 \n",
    "        self.batch_size = 24\n",
    "        self.deterministic = 1 \n",
    "        self.base_lr = 1e-3\n",
    "        self.img_size = [256,256] \n",
    "        self.seed = 42 \n",
    "        self.num_classes = 4 \n",
    "        self.patch_size = 16\n",
    "        self.mask_ratio = 0.5\n",
    "\n",
    "        # label and unlabel \n",
    "        self.labeled_bs = 12\n",
    "        self.label_num = 7 \n",
    "        self.u_weight = 0.5 \n",
    "\n",
    "        # Cost \n",
    "        self.gpu = '0' \n",
    "        self.consistency = 0.1\n",
    "        self.consistency_rampup = 200.0 \n",
    "        self.magnitude = '6.0' \n",
    "        self.s_param = 6\n",
    "\n",
    "        # Caussl parameters \n",
    "        self.consistency_type = 'mse'   \n",
    "        self.max_step = 60 \n",
    "        self.min_step = 60 \n",
    "        self.start_step1 = 50 \n",
    "        self.start_step2 = 50 \n",
    "        self.cofficient = 3.0 \n",
    "        self.max_iteration = 5000 \n",
    "        self.thres_iteration = 20 \n",
    "\n",
    "args = params() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56846d1",
   "metadata": {},
   "source": [
    "# ACDC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f5d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACDCDataset(Dataset): \n",
    "    \"\"\"\n",
    "    Use to load ACDC dataset \n",
    "    This dataset support 3 modes (strings only): \n",
    "    - train_lab: labeled training data \n",
    "    - train_unlab: unlabeled traning data \n",
    "    - val: validation data \n",
    "\n",
    "    Parameters: \n",
    "    - base_dir (str): folder save data \n",
    "    - split (str): type of data want to load\n",
    "    - reservse (str): use to reverse the index of data \n",
    "    - transform (torchvision.transform): the transform apply for data \n",
    "    \"\"\"\n",
    "    def __init__(self, base_dir, split='train_lab', reverse=None, transform=None): \n",
    "        super(ACDCDataset, self).__init__() \n",
    "        self.base_dir = base_dir\n",
    "        self.split = split\n",
    "        self.reverse = reverse\n",
    "        self.transform = transform\n",
    "        self.sample_list = []\n",
    "\n",
    "        # Read the file \n",
    "        if self.split == 'train_lab': \n",
    "            with open(os.path.join(self.base_dir, 'train_lab.list'), 'r') as file: \n",
    "                self.sample_list = file.readlines() \n",
    "        elif self.split == 'train_unlab': \n",
    "            with open(os.path.join(self.base_dir, 'train_unlab.list'), 'r') as file: \n",
    "                self.sample_list = file.readlines() \n",
    "        elif self.split == 'val': \n",
    "            with open(os.path.join(self.base_dir, 'val.list'), 'r') as file: \n",
    "                self.sample_list = file.readlines() \n",
    "        elif self.split == 'reconstruct': \n",
    "            with open(os.path.join(self.base_dir, 'all_slices.list'), 'r') as file: \n",
    "                self.sample_list = file.readlines() \n",
    "        else: \n",
    "            raise ValueError(f'Split: {self.split} is not support for ACDC dataset')\n",
    "        \n",
    "        self.sample_list = [item.replace('\\n', '') for item in self.sample_list]\n",
    "        print(f'Mode: {self.split}: {len(self.sample_list)} samples in total')\n",
    "\n",
    "\n",
    "    def __len__(self): \n",
    "        if (self.split == 'train_lab') | (self.split == 'train_unlab'): \n",
    "            return len(self.sample_list) * 10 # Why use it ???? \n",
    "        \n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        case = self.sample_list[idx%len(self.sample_list)] # Avoid problem of __len__ \n",
    "        if self.reverse: \n",
    "            case = self.sample_list[len(self.sample_list) - idx%len(self.sample_list) - 1] \n",
    "\n",
    "        # read the file \n",
    "        if (self.split == 'train_lab') | (self.split == 'train_unlab') | (self.split == 'reconstruct'): \n",
    "            h5f = h5py.File((self.base_dir + f'/data/slices/{case}.h5'), 'r')         \n",
    "        elif (self.split == 'val'): \n",
    "            h5f = h5py.File((self.base_dir + f'/data/{case}.h5'), 'r')\n",
    "        \n",
    "        image = h5f['image'][:]\n",
    "        label = h5f['label'][:]\n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        if self.transform: \n",
    "            sample = self.transform(sample)\n",
    "        image_, label_ = sample['image'], sample['label']\n",
    "        return image_, label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0679e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rot_flip(image, label): \n",
    "    \"\"\"\n",
    "    Random rotate and Random flip \n",
    "    \"\"\"\n",
    "    \n",
    "    # Random rotate\n",
    "    k = np.random.randint(0, 4) \n",
    "    image = np.rot90(image, k)\n",
    "    label = np.rot90(label, k)\n",
    "\n",
    "    # Random flip \n",
    "    axis = np.random.randint(0, 2)\n",
    "    image = np.flip(image, axis).copy() \n",
    "    label = np.flip(label, axis).copy() \n",
    "\n",
    "    return image, label \n",
    "\n",
    "def random_rotate(image, label):\n",
    "    angle = np.random.randint(-20, 20) \n",
    "    image = rotate(image, angle, order= 0, reshape= False)\n",
    "    label = rotate(label,angle, order=0, reshape= False )\n",
    "    return image, label\n",
    "\n",
    "\n",
    "class RandomGenerator: \n",
    "    def __init__(self, output_size): \n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        if np.random.random() > 0.5: \n",
    "            image, label = random_rot_flip(image, label)\n",
    "        \n",
    "        if np.random.random() > 0.5: \n",
    "            image, label = random_rotate(image, label) \n",
    "        \n",
    "        # Zoom image to -> [256,256]\n",
    "        x,y = image.shape\n",
    "        image = zoom(image, (self.output_size[0] / x, self.output_size[1] / y), order= 0)\n",
    "        label = zoom(label, (self.output_size[0] /x , self.output_size[1] / y), order= 0)\n",
    "\n",
    "        # Convert to pytorch \n",
    "        imageTensor = torch.from_numpy(image.astype(np.float32)).unsqueeze(0) # image.shape = (1, H, W)\n",
    "        labelTensor = torch.from_numpy(label.astype(np.uint8)) # label.shape = (H, W)\n",
    "        sample = {'image': imageTensor, 'label': labelTensor}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1509d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_once(indices): \n",
    "    \"\"\"\n",
    "    Permutate the iterable once \n",
    "    (permutate the labeled_idxs once)\n",
    "    \"\"\"\n",
    "    return np.random.permutation(indices) \n",
    "\n",
    "def iterate_externally(indices): \n",
    "    \"\"\"\n",
    "    Create an infinite iterator that repeatedly permutes the indices.\n",
    "    ( permutate the unlabeled_idxs to make different)\n",
    "    \"\"\"\n",
    "    def infinite_shuffles(): \n",
    "        while True: \n",
    "            yield np.random.permutation(indices)\n",
    "            \n",
    "    return itertools.chain.from_iterable(infinite_shuffles())\n",
    "\n",
    "def grouper(iterable, n): \n",
    "    args = [iter(iterable)] * n \n",
    "    return zip(*args)\n",
    "\n",
    "class TwoStreamBatchSampler(Sampler): \n",
    "    def __init__(self, primary_indicies, secondary_indicies, batchsize, secondary_batchsize): \n",
    "        self.primary_indicies = primary_indicies\n",
    "        self.secondary_indicies = secondary_indicies\n",
    "        self.primary_batchsize = batchsize - secondary_batchsize\n",
    "        self.secondary_batchsize = secondary_batchsize\n",
    "\n",
    "        assert len(self.primary_indicies) >= self.primary_batchsize > 0 \n",
    "        assert len(self.secondary_indicies) >= self.secondary_batchsize > 0 \n",
    "\n",
    "    def __iter__(self):\n",
    "        primary_iter = iterate_once(self.primary_indicies)\n",
    "        secondary_iter = iterate_externally(self.secondary_indicies)\n",
    "\n",
    "        return (\n",
    "            primary_batch + secondary_batch\n",
    "            for (primary_batch, secondary_batch) \n",
    "            in zip(grouper(primary_iter, self.primary_batchsize),\n",
    "                   grouper(secondary_iter, self.secondary_batchsize))\n",
    "        )\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.primary_indicies) // self.primary_batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41190a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data \n",
    "def patients_to_slices(dataset, patients_num): \n",
    "    ref_dict = {} \n",
    "    if \"ACDC\" in dataset: \n",
    "        ref_dict = {'1': 32, '3': 68, '7': 136, '14': 256, '21': 396, '28': 512, '35': 664, '70': 1312}\n",
    "    else:\n",
    "        print('Error')\n",
    "    \n",
    "    return ref_dict[str(patients_num)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4acbeb",
   "metadata": {},
   "source": [
    "# BCP Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc4cda31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module): \n",
    "    def __init__(self, n_classes): \n",
    "        super(DiceLoss, self).__init__() \n",
    "        self.n_classes = n_classes\n",
    "    \n",
    "    def _one_hot_encoder(self, input_tensor): # torch.nn.functional.one_hot()\n",
    "        \"\"\"\n",
    "        Apply one-hot encoder for input_tensor \n",
    "        Parameters: \n",
    "            - input_tensor.shape = (batchsize,1, H, W), the target image\n",
    "        \"\"\"\n",
    "        tensor_list = [] \n",
    "        for i in range(self.n_classes): \n",
    "            temp_prob = input_tensor == i * torch.ones_like(input_tensor)\n",
    "            tensor_list.append(temp_prob)\n",
    "        output_tensor = torch.cat(tensor_list, dim= 1)\n",
    "        return output_tensor.float() \n",
    "    \n",
    "    def _dice_loss(self, score, target): \n",
    "        target = target.float() \n",
    "        smooth = 1e-10 \n",
    "        \n",
    "        intersection = torch.sum(score * target)\n",
    "        union = torch.sum(score* score) + torch.sum(target*target)\n",
    "        dice = ( 2*intersection + smooth) / (union + smooth)\n",
    "        loss = 1 - dice \n",
    "        return loss \n",
    "    \n",
    "    def _dice_mask_loss(self, score, target, mask): \n",
    "        target = target.float() \n",
    "        mask = mask.float() \n",
    "        smooth = 1e-10 \n",
    "\n",
    "        intersection = torch.sum(score * target * mask)\n",
    "        union = torch.sum(score * score * mask ) + torch.sum(target * target * mask)\n",
    "        dice = (2*intersection + smooth) / (union + smooth)\n",
    "        loss = 1 - dice \n",
    "        return loss \n",
    "\n",
    "    def forward(self, inputs, target, mask= None, weight= None, softmax= False): \n",
    "        if softmax: \n",
    "            inputs = torch.softmax(inputs, dim= 1) \n",
    "        \n",
    "        target = self._one_hot_encoder(target)\n",
    "\n",
    "        # weight \n",
    "        if weight is  None: \n",
    "            weight = [1] * self.n_classes\n",
    "        \n",
    "        assert inputs.size() == target.size(), 'predict and target shape do not match'\n",
    "        class_wise_dice = [] \n",
    "        loss = 0.0 \n",
    "        if mask is not None: \n",
    "            mask = mask.repeat(1, self.n_classes, 1, 1).type(torch.float32)\n",
    "            for i in range(0, self.n_classes): \n",
    "                dice = self._dice_mask_loss(inputs[:, i], target[:, i], mask[:, i])\n",
    "                class_wise_dice.append( 1.0 - dice.item())\n",
    "                loss += dice * weight[i]\n",
    "\n",
    "        else: \n",
    "            for i in range(0, self.n_classes): \n",
    "                dice = self._dice_loss(inputs[:, i], target[:, i]) \n",
    "                class_wise_dice.append(1.0 - dice.item())\n",
    "                loss += dice * weight[i] \n",
    "        \n",
    "        return loss / self.n_classes\n",
    "    \n",
    "\n",
    "    \n",
    "dice_loss = DiceLoss(n_classes= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f3fa78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_loss(output, img_l, patch_l, mask, l_weight=1.0, u_weight=0.5, unlab=False):\n",
    "    CE = nn.CrossEntropyLoss(reduction='none')\n",
    "    img_l, patch_l = img_l.type(torch.int64), patch_l.type(torch.int64)\n",
    "    output_soft = F.softmax(output, dim=1)\n",
    "    image_weight, patch_weight = l_weight, u_weight\n",
    "    if unlab:\n",
    "        image_weight, patch_weight = u_weight, l_weight\n",
    "    patch_mask = 1 - mask\n",
    "    loss_dice = dice_loss(output_soft, img_l.unsqueeze(1), mask.unsqueeze(1)) * image_weight\n",
    "    loss_dice += dice_loss(output_soft, patch_l.unsqueeze(1), patch_mask.unsqueeze(1)) * patch_weight\n",
    "    loss_ce = image_weight * (CE(output, img_l) * mask).sum() / (mask.sum() + 1e-16) \n",
    "    loss_ce += patch_weight * (CE(output, patch_l) * patch_mask).sum() / (patch_mask.sum() + 1e-16)#loss = loss_ce\n",
    "    return loss_dice, loss_ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e2a24",
   "metadata": {},
   "source": [
    "# MAE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a026fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(X_rec, X_orig, mask, lam=0.1):\n",
    "    \"\"\"\n",
    "    Compute full L_REC loss as in the SDCL paper:\n",
    "    mask: 1 for visible, 0 for masked\n",
    "    lam: lambda weight for visible region\n",
    "    \"\"\"\n",
    "    loss_masked = ((1 - mask) * (X_rec - X_orig) ** 2).sum()\n",
    "    loss_visible = (mask * (X_rec - X_orig) ** 2).sum()\n",
    "    \n",
    "    total_pixels = X_orig.numel()\n",
    "    loss = (loss_masked + lam * loss_visible) / total_pixels\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c0465",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2184280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ACDC_2DLargestCC(segmentation):\n",
    "    batch_list = []\n",
    "    N = segmentation.shape[0]\n",
    "    for i in range(0, N):\n",
    "        class_list = []\n",
    "        for c in range(1, 4):\n",
    "            temp_seg = segmentation[i] #== c *  torch.ones_like(segmentation[i])\n",
    "            temp_prob = torch.zeros_like(temp_seg)\n",
    "            temp_prob[temp_seg == c] = 1\n",
    "            temp_prob = temp_prob.detach().cpu().numpy()\n",
    "            labels = label(temp_prob)          \n",
    "            if labels.max() != 0:\n",
    "                largestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1\n",
    "                class_list.append(largestCC * c)\n",
    "            else:\n",
    "                class_list.append(temp_prob)\n",
    "        \n",
    "        n_batch = class_list[0] + class_list[1] + class_list[2]\n",
    "        batch_list.append(n_batch)\n",
    "\n",
    "    return torch.Tensor(batch_list).cuda()\n",
    "    \n",
    "def get_ACDC_masks(output, nms=0):\n",
    "    probs = F.softmax(output, dim=1)\n",
    "    _, probs = torch.max(probs, dim=1)\n",
    "    if nms == 1:\n",
    "        probs = get_ACDC_2DLargestCC(probs)      \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c230ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_net_opt(net, optimizer, path):\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'optim': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, str(path))\n",
    "\n",
    "def load_net(net, path):\n",
    "    state = torch.load(str(path))\n",
    "    net.load_state_dict(state['net']) \n",
    "\n",
    "def load_net_opt(net, optimizer, path): \n",
    "    state = torch.load(str(path))\n",
    "    net.load_state_dict(state['net'])\n",
    "    optimizer.load_state_dict(state['optim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6074688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(img):\n",
    "    batch_size, channel, img_x, img_y = img.shape[0], img.shape[1], img.shape[2], img.shape[3]\n",
    "    loss_mask = torch.ones(batch_size, img_x, img_y).cuda()\n",
    "    mask = torch.ones(img_x, img_y).cuda()\n",
    "    patch_x, patch_y = int(img_x*2/3), int(img_y*2/3)\n",
    "    w = np.random.randint(0, img_x - patch_x)\n",
    "    h = np.random.randint(0, img_y - patch_y)\n",
    "    mask[w:w+patch_x, h:h+patch_y] = 0\n",
    "    loss_mask[:, w:w+patch_x, h:h+patch_y] = 0\n",
    "    return mask.long(), loss_mask.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf7966be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_rampup(current, rampup_length):\n",
    "    if rampup_length == 0: \n",
    "        return 1.0 \n",
    "    else:\n",
    "        current = np.clip(current, 0, rampup_length)\n",
    "        phase = 1 - (current / rampup_length)\n",
    "        return float(np.exp(-5 * phase * phase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89c022d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean-Teacher compomnent \n",
    "def get_current_consistency_weight(epoch, args): \n",
    "    return 5 * args.consistency + sigmoid_rampup(epoch, args.consistency_rampup)\n",
    "\n",
    "def update_model_ema(model, ema_model, alpha): \n",
    "    model_state = model.state_dict() \n",
    "    model_ema_state = ema_model.state_dict()\n",
    "    new_dict = {}\n",
    "\n",
    "    for key in model_state:\n",
    "        new_dict[key] = alpha * model_ema_state[key] + (1 - alpha) * model_state[key]\n",
    "\n",
    "    ema_model.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "183cdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_percase(pred, gt):\n",
    "    pred[pred > 0] = 1\n",
    "    gt[gt > 0] = 1\n",
    "    if pred.sum() > 0:\n",
    "        dice = metric.binary.dc(pred, gt)\n",
    "        hd95 = metric.binary.hd95(pred, gt)\n",
    "        return dice, hd95\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "\n",
    "def test_single_volume(image, label, model, classes, img_size=[256, 256]):\n",
    "    image, label = image.squeeze(0).cpu().detach().numpy(), label.squeeze(0).cpu().detach().numpy()\n",
    "    prediction = np.zeros_like(label)\n",
    "    for ind in range(image.shape[0]):\n",
    "        slice = image[ind, :, :]\n",
    "        x, y = slice.shape[0], slice.shape[1]\n",
    "        slice = zoom(slice, (img_size[0] / x, img_size[1] / y), order=0)\n",
    "        input = torch.from_numpy(slice).unsqueeze(0).unsqueeze(0).float().cuda()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model.forward_segmentation(input)\n",
    "            if len(output)>1:\n",
    "                output = output[0]\n",
    "            out = torch.argmax(torch.softmax(output, dim=1), dim=1).squeeze(0)\n",
    "            out = out.cpu().detach().numpy()\n",
    "            pred = zoom(out, (x / img_size[0], y / img_size[1]), order=0)\n",
    "            prediction[ind] = pred\n",
    "    metric_list = []\n",
    "    for i in range(1, classes):\n",
    "        metric_list.append(calculate_metric_percase(prediction == i, label == i))\n",
    "    return metric_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff74fc",
   "metadata": {},
   "source": [
    "# random masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5914c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_mask(x, patch_size, mask_ratio):\n",
    "        N = x.shape[0]\n",
    "        L = (x.shape[2] // patch_size) ** 2\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "        noise = torch.randn(N, L, device=x.device)\n",
    "\n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # generate the binary mask: 0 is keep 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "        return mask\n",
    "\n",
    "\n",
    "def upsample_mask(mask, patch_size, H, W):\n",
    "    p = int(mask.shape[1] ** 0.5)\n",
    "    mask = mask.reshape(-1, p, p).unsqueeze(1)  # [B, 1, h, w]\n",
    "    mask = mask.repeat_interleave(patch_size, 2).repeat_interleave(patch_size, 3)\n",
    "    return mask  # [B, 1, H, W]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5a1c4",
   "metadata": {},
   "source": [
    "# Unet backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ec672d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRN_ChannelFirst(nn.Module):\n",
    "    def __init__(self, num_channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        gx = torch.norm(x, p=2, dim=(2, 3), keepdim=True)   # L2 norm [B, C, 1, 1]\n",
    "        nx = gx / (gx.mean(dim=1, keepdim=True) + self.eps) # Normalize across channels\n",
    "        return self.gamma * (x * nx) + self.beta + x # Residual + learnable modulation\n",
    "\n",
    "\n",
    "# Build again UNet backbone \n",
    "class ConvBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    Two convolution block with batchnorm and leakyrelu \n",
    "    Dont change the output size  \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, out_channel, dropout_p, use_grn=True): \n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.use_grn = use_grn\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channel), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "    \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channel), \n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.grn = GRN_ChannelFirst(out_channel)  # Add GRN layer\n",
    "\n",
    "    def forward(self, x, apply_grn=False): \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        if apply_grn:\n",
    "            x = self.grn(x)\n",
    "        return x\n",
    "\n",
    "class DownBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    Downsample follow by ConvBlock\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, out_channel, dropout_p): \n",
    "        super(DownBlock, self).__init__() \n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size= 2)\n",
    "        self.conv = ConvBlock(in_channel, out_channel, dropout_p)\n",
    "\n",
    "\n",
    "        # self.maxpool_conv = nn.Sequential(\n",
    "        #     nn.MaxPool2d(kernel_size= 2), \n",
    "        #     ConvBlock(in_channel, out_channel, dropout_p)\n",
    "        # )\n",
    "\n",
    "    def forward(self, x, apply_grn=False):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x, apply_grn = apply_grn) \n",
    "        return x\n",
    "    \n",
    "\n",
    "class UpBlock(nn.Module): # Check if have problem  \n",
    "    def __init__(self, in_channel1, in_channel2, out_channel, dropout_p): \n",
    "        super(UpBlock, self).__init__() \n",
    "\n",
    "        self.convx1 = nn.Conv2d(in_channel1, in_channel2, kernel_size= 1) # WRD\n",
    "        self.up = nn.Upsample(scale_factor= 2, mode= 'bilinear', align_corners= True)\n",
    "        self.conv = ConvBlock(in_channel2 * 2, out_channel, dropout_p)\n",
    "         \n",
    "    def forward(self, x1, x2, apply_grn=False): \n",
    "        x1 = self.convx1(x1)\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim= 1)\n",
    "        return self.conv(x, apply_grn=apply_grn) \n",
    "\n",
    "class Encoder(nn.Module): \n",
    "    def __init__(self, params): \n",
    "        super(Encoder, self).__init__()\n",
    "        self.params = params\n",
    "        self.in_chs = self.params['in_chs']\n",
    "        self.ft_chs = self.params['ft_chs']\n",
    "        self.n_class = self.params['num_class']\n",
    "        self.dropout = self.params['dropout']\n",
    "        assert (len(self.ft_chs) == 5)\n",
    "\n",
    "        self.conv = ConvBlock(self.in_chs, self.ft_chs[0], dropout_p= self.dropout[0])\n",
    "        self.down1 = DownBlock(self.ft_chs[0], self.ft_chs[1], self.dropout[1])\n",
    "        self.down2 = DownBlock(self.ft_chs[1], self.ft_chs[2], self.dropout[2])\n",
    "        self.down3 = DownBlock(self.ft_chs[2], self.ft_chs[3], self.dropout[3])\n",
    "        self.down4 = DownBlock(self.ft_chs[3], self.ft_chs[4], self.dropout[4])\n",
    "\n",
    "    def forward(self, x, apply_grn=False): \n",
    "        x0 = self.conv(x, apply_grn=apply_grn) \n",
    "        x1 = self.down1(x0, apply_grn=apply_grn) \n",
    "        x2 = self.down2(x1, apply_grn=apply_grn) \n",
    "        x3 = self.down3(x2, apply_grn=apply_grn)\n",
    "        x4 = self.down4(x3, apply_grn=apply_grn)\n",
    "        return [x0, x1, x2, x3, x4]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "    def __init__(self, params): \n",
    "        super(Decoder, self).__init__() \n",
    "        self.params = params \n",
    "        self.in_chs = self.params['in_chs']\n",
    "        self.ft_chs = self.params['ft_chs']\n",
    "        self.n_class = self.params['num_class']\n",
    "        assert (len(self.ft_chs) == 5)\n",
    "\n",
    "        self.up1 = UpBlock(self.ft_chs[4], self.ft_chs[3], self.ft_chs[3], dropout_p= 0.0)\n",
    "        self.up2 = UpBlock(self.ft_chs[3], self.ft_chs[2], self.ft_chs[2], dropout_p= 0.0)\n",
    "        self.up3 = UpBlock(self.ft_chs[2], self.ft_chs[1], self.ft_chs[1], dropout_p= 0.0)\n",
    "        self.up4 = UpBlock(self.ft_chs[1], self.ft_chs[0], self.ft_chs[0], dropout_p= 0.0)\n",
    "        self.out_conv = nn.Conv2d(self.ft_chs[0], self.n_class, kernel_size= 3, padding= 1)\n",
    "\n",
    "\n",
    "    def forward(self, feature, apply_grn = False): \n",
    "        x0 = feature[0] \n",
    "        x1 = feature[1] \n",
    "        x2 = feature[2] \n",
    "        x3 = feature[3]\n",
    "        x4 = feature[4] \n",
    "\n",
    "        x = self.up1(x4, x3, apply_grn) \n",
    "        x = self.up2(x, x2, apply_grn)\n",
    "        x = self.up3(x, x1, apply_grn)\n",
    "        x_last = self.up4(x, x0, apply_grn)\n",
    "        output = self.out_conv(x_last)\n",
    "        return output, x_last\n",
    "    \n",
    "class UNet2d(nn.Module): \n",
    "    def __init__(self, in_chs, class_num=None, recon=False): \n",
    "        super(UNet2d, self).__init__() \n",
    "        self.recon = recon\n",
    "        self.params = {\n",
    "            'in_chs': in_chs, \n",
    "            'ft_chs': [16, 32, 64, 128, 256], \n",
    "            'dropout': [0.05, 0.1, 0.2, 0.3, 0.5], \n",
    "            'num_class': class_num if not recon else in_chs\n",
    "        }\n",
    "\n",
    "        self.encoder = Encoder(self.params) \n",
    "        self.decoder = Decoder(self.params)\n",
    "\n",
    "        # ⬇️ final projection layer for reconstruction\n",
    "        if recon:\n",
    "            self.out_conv = nn.Conv2d(self.params['ft_chs'][0], in_chs, kernel_size=3, padding=1)\n",
    "        else:\n",
    "            self.out_conv = None\n",
    "\n",
    "    def forward(self, x): \n",
    "        features = self.encoder(x) \n",
    "        x_out, x_last = self.decoder(features)\n",
    "\n",
    "        if self.recon:\n",
    "            return self.out_conv(x_last)  # Ensure output shape = input shape\n",
    "        else:\n",
    "            return x_out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e73c6",
   "metadata": {},
   "source": [
    "# MBCP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c67d0695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBCP(nn.Module):\n",
    "    def __init__(self, encoder_type='unet', in_chs=1, class_num=4, use_recon = False):\n",
    "        super(MBCP, self).__init__()\n",
    "\n",
    "        self.shared_params = {\n",
    "            'in_chs': in_chs,\n",
    "            'ft_chs': [16, 32, 64, 128, 256],\n",
    "            'dropout': [0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "            'num_class': class_num  # For seg decoder\n",
    "        }\n",
    "\n",
    "        # Shared encoder\n",
    "        self.encoder = Encoder(self.shared_params)\n",
    "\n",
    "        # Segmentation decoder\n",
    "        self.decoder_seg = Decoder(self.shared_params)\n",
    "\n",
    "        # Reconstruction decoder (output RGB)\n",
    "        mae_params = self.shared_params.copy()\n",
    "        mae_params['num_class'] = in_chs\n",
    "        self.decoder_recon = Decoder(mae_params)\n",
    "\n",
    "        \n",
    "    def forward_segmentation(self, x_bcp):\n",
    "        # shared encoder\n",
    "        feat_bcp = self.encoder(x_bcp)\n",
    "        seg_out, _ = self.decoder_seg(feat_bcp)  # Segmentation output [B, class_num, H, W] \n",
    "        return seg_out\n",
    "    \n",
    "    def forward_reconstruction(self, x_masked):\n",
    "        feat_masked = self.encoder(x_masked)\n",
    "        rec_out, _ = self.decoder_recon(feat_masked) # Reconstruction output [B, in_chns, H, W]\n",
    "        return rec_out  \n",
    "\n",
    "    def forward(self, x):  \n",
    "        return self.forward_segmentation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f03c4b",
   "metadata": {},
   "source": [
    "# engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c313fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBCP_net(role, in_chs=1, class_num=4, backbone='unet', ema=False, use_recon=False):\n",
    "    if role == 'mbcp':\n",
    "        # encoder shared, 2 decoder forn segmentation and reconstruction\n",
    "        model = MBCP(encoder_type=backbone, in_chs=in_chs, class_num=class_num, use_recon=use_recon).cuda()\n",
    "    \n",
    "    \n",
    "    elif role == 'teacher':\n",
    "        model = MBCP(encoder_type = backbone, in_chs=in_chs, class_num=class_num, use_recon=use_recon).cuda() # ignore decoder mae\n",
    "        if ema:\n",
    "            for param in model.parameters():\n",
    "                param.detach_() \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4431525",
   "metadata": {},
   "source": [
    "# pre train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b605c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "def pre_train(args, snapshot_path):\n",
    "    base_lr = args.base_lr\n",
    "    num_classes = args.num_classes\n",
    "    max_iterations = args.pretrain_iterations\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu \n",
    "    pre_trained_model = os.path.join(snapshot_path, '{}_best_model.pth'.format(args.model))\n",
    "    labeled_sub_bs, unlabeled_sub_bs = int(args.labeled_bs / 2), int((args.batch_size - args.labeled_bs)/2)\n",
    "\n",
    "    def worker_init_fn(worker_id):\n",
    "        random.seed(args.seed + worker_id)\n",
    "\n",
    "    # load dataset \n",
    "    db_train = ACDCDataset(base_dir= args.root_dir, split= 'train_lab',\n",
    "                        transform= transforms.Compose([RandomGenerator(args.img_size)]))\n",
    "    \n",
    "    db_val = ACDCDataset(base_dir= args.root_dir, split= 'val')\n",
    "\n",
    "    total_slices = len(db_train)\n",
    "    labeled_slices = patients_to_slices(args.root_dir, args.label_num)\n",
    "    print(f'Total slice is {total_slices}, Labeled slice is {labeled_slices}')\n",
    "\n",
    "    # Create batch_sampler \n",
    "    labeled_idxs = list(range(0, labeled_slices))\n",
    "    unlabeled_idxs = list(range(labeled_slices, total_slices))\n",
    "    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, args.batch_size, args.batch_size - args.labeled_bs)\n",
    "\n",
    "    # Create dataloader \n",
    "    trainloader = DataLoader(db_train, batch_sampler= batch_sampler, num_workers= 4, pin_memory= True, worker_init_fn= worker_init_fn)\n",
    "    valloader = DataLoader(db_val, batch_size= 1, shuffle= False, num_workers=1)\n",
    "\n",
    "    # Define model \n",
    "    model = MBCP_net(role='mbcp', in_chs=1, class_num=num_classes, use_recon=False)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr= base_lr, weight_decay= 0.0001)\n",
    "\n",
    "    writer = SummaryWriter(snapshot_path + '/log')\n",
    "    logging.info('Start pre-training')\n",
    "    logging.info(f'{len(trainloader)} iterations per epoch')\n",
    "\n",
    "    # training process\n",
    "    model.train() \n",
    "    iter_num = 0 \n",
    "    max_epoch = max_iterations // len(trainloader) + 1 \n",
    "    best_performance = 0.0 \n",
    "    best_hd = 100.0\n",
    "    iterator = tqdm(range(max_epoch), ncols= 70)\n",
    "    \n",
    "    for _ in iterator: \n",
    "        for _, sampled_batch in enumerate(trainloader): \n",
    "            volume_batch, label_batch = sampled_batch\n",
    "            volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda() \n",
    "\n",
    "            img_a, img_b = volume_batch[: labeled_sub_bs], volume_batch[labeled_sub_bs : args.labeled_bs]\n",
    "            lab_a, lab_b = label_batch[: labeled_sub_bs], label_batch[labeled_sub_bs : args.labeled_bs]\n",
    "            img_mask, loss_mask = generate_mask(img_a)\n",
    "            gt_mixl = lab_a * img_mask + lab_b * ( 1- img_mask)\n",
    "\n",
    "            #-- original \n",
    "            net_input = img_a * img_mask + img_b * ( 1 - img_mask)  \n",
    "            out_mixl = model.forward_segmentation(net_input)\n",
    "            loss_dice, loss_ce = mix_loss(out_mixl, lab_a, lab_b, loss_mask,u_weight= 1.0, unlab= True )\n",
    "            loss = (loss_dice + loss_ce )/2 \n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            iter_num += 1 \n",
    "\n",
    "            writer.add_scalar('info/total_loss', loss, iter_num)\n",
    "            writer.add_scalar('info/mix_dice', loss_dice,iter_num )\n",
    "            writer.add_scalar('info/mix_ce', loss_ce, iter_num)\n",
    "\n",
    "            logging.info('iteration %d: loss %f, mix_dice: %f, mix_ce: %f'%(iter_num, loss, loss_dice, loss_ce))\n",
    "            if iter_num % 20 == 0: \n",
    "                image = net_input[1, 0:1, :, :]\n",
    "                writer.add_image('pre_train/Mixed_Image', image, iter_num)\n",
    "                outputs = torch.argmax(torch.softmax(out_mixl, dim=1), dim=1, keepdim= True)\n",
    "                writer.add_image('pre_train/Mixed_Prediction', outputs[1, ...]*50, iter_num)\n",
    "                labs = gt_mixl[1, ...].unsqueeze(0) * 50 \n",
    "                writer.add_image('pre_train/Mixed_GroundTruth', labs, iter_num)\n",
    "            \n",
    "            # Evaluate after 200 epoch ! \n",
    "            if iter_num > 0 and iter_num % 200 == 0: \n",
    "                model.eval() \n",
    "                metric_list = 0.0 \n",
    "                for _, sampled_batch in enumerate(valloader):\n",
    "                    image_batch, label_batch = sampled_batch\n",
    "                    metric_i = test_single_volume(image_batch, label_batch, model, classes= num_classes)\n",
    "                    metric_list += np.array(metric_i)\n",
    "                \n",
    "                metric_list = metric_list / len(db_val)\n",
    "                \n",
    "    \n",
    "                for class_i in range(num_classes - 1 ): \n",
    "                    writer.add_scalar('info/val_{}_dice'.format(class_i + 1), metric_list[class_i, 0], iter_num)\n",
    "                    writer.add_scalar('infor/val_{}_hd'.format(class_i + 1), metric_list[class_i, 1], iter_num)\n",
    "                \n",
    "                performance = np.mean(metric_list, axis=0)[0]\n",
    "                writer.add_scalar('info/val_mean_dice', performance, iter_num)\n",
    "            \n",
    "                if performance > best_performance: \n",
    "                    best_performance = performance\n",
    "                    save_model_path = os.path.join(snapshot_path, 'iter_{}_dice_{}.pth'.format(iter_num, round(best_performance,4)))\n",
    "                    save_best_path = os.path.join(snapshot_path, '{}_best_model.pth'.format(args.model))\n",
    "                    save_net_opt(model, optimizer, save_model_path)\n",
    "                    save_net_opt(model, optimizer, save_best_path)\n",
    "                \n",
    "                logging.info('iteration %d : mean dice : %f'%(iter_num, performance))\n",
    "                model.train() \n",
    "            \n",
    "            if iter_num >= max_iterations: \n",
    "                break \n",
    "        \n",
    "        if iter_num >= max_iterations: \n",
    "            iterator.close() \n",
    "            break \n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6debe6",
   "metadata": {},
   "source": [
    "# self train (BCP + MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee8879bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_train(args ,pre_snapshot_path, snapshot_path):\n",
    "    base_lr = args.base_lr\n",
    "    num_classes = args.num_classes\n",
    "    max_iterations = args.selftrain_iterations\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "    pre_trained_model = os.path.join(pre_snapshot_path,'{}_best_model.pth'.format(args.model))\n",
    "    labeled_sub_bs, unlabeled_sub_bs = int(args.labeled_bs/2), int((args.batch_size-args.labeled_bs) / 2)\n",
    "\n",
    "    # model\n",
    "    model = MBCP_net(role='mbcp', in_chs=1, class_num=num_classes, use_recon = True).cuda()\n",
    "    ema_model = MBCP_net(role='teacher', in_chs=1, class_num=num_classes, ema=True, use_recon = False).cuda()\n",
    "\n",
    "    def worker_init_fn(worker_id):\n",
    "        random.seed(args.seed + worker_id)\n",
    "\n",
    "    db_train = ACDCDataset(base_dir=args.root_dir,\n",
    "                            split=\"train_lab\",\n",
    "                            transform=transforms.Compose([RandomGenerator(args.img_size)]))\n",
    "    \n",
    "    db_mae = ACDCDataset(base_dir=args.root_dir, split='reconstruct',\n",
    "                         transform=transforms.Compose([RandomGenerator(args.img_size)]))\n",
    "\n",
    "\n",
    "    db_val = ACDCDataset(base_dir=args.root_dir, split=\"val\")\n",
    "\n",
    "\n",
    "    total_slices = len(db_train)\n",
    "    labeled_slice = patients_to_slices(args.root_dir,args.label_num)\n",
    "    print(\"Total slices is: {}, labeled slices is:{}\".format(total_slices, labeled_slice))\n",
    "    labeled_idxs = list(range(0, labeled_slice))\n",
    "    unlabeled_idxs = list(range(labeled_slice, total_slices))\n",
    "    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, args.batch_size, args.batch_size-args.labeled_bs)\n",
    "\n",
    "    trainloader = DataLoader(db_train, batch_sampler=batch_sampler, num_workers=4, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "\n",
    "    valloader = DataLoader(db_val, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "    mae_batch, _ = next(iter(DataLoader(db_mae, batch_size=args.batch_size, shuffle=True)))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=0.0001)\n",
    "    load_net(ema_model, pre_trained_model)\n",
    "    load_net_opt(model, optimizer, pre_trained_model) #\n",
    "    logging.info(\"Loaded from {}\".format(pre_trained_model))\n",
    "\n",
    "    writer = SummaryWriter(snapshot_path + '/log')\n",
    "    logging.info(\"Start self_training\")\n",
    "    logging.info(\"{} iterations per epoch\".format(len(trainloader)))\n",
    "\n",
    "    model.train()\n",
    "    ema_model.train()\n",
    "\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    iter_num = 0\n",
    "    max_epoch = max_iterations // len(trainloader) + 1\n",
    "    best_performance = 0.0\n",
    "    best_hd = 100\n",
    "    iterator = tqdm(range(max_epoch), ncols=70)\n",
    "    for _ in iterator:\n",
    "        for _, sampled_batch in enumerate(trainloader):\n",
    "            \n",
    "            volume_batch, label_batch = sampled_batch\n",
    "            volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda()\n",
    "            \n",
    "            # mae input\n",
    "            mask = gen_random_mask(mae_batch, patch_size=args.patch_size, mask_ratio=args.mask_ratio)\n",
    "            mask_up = upsample_mask(mask, patch_size=args.patch_size, H = mae_batch.shape[2], W = mae_batch.shape[3])\n",
    "            masked_img = mae_batch * (1 - mask_up)\n",
    "\n",
    "            img_a, img_b = volume_batch[:labeled_sub_bs], volume_batch[labeled_sub_bs:args.labeled_bs]\n",
    "            uimg_a, uimg_b = volume_batch[args.labeled_bs:args.labeled_bs + unlabeled_sub_bs], volume_batch[args.labeled_bs + unlabeled_sub_bs:]\n",
    "            ulab_a, ulab_b = label_batch[args.labeled_bs:args.labeled_bs + unlabeled_sub_bs], label_batch[args.labeled_bs + unlabeled_sub_bs:]\n",
    "            lab_a, lab_b = label_batch[:labeled_sub_bs], label_batch[labeled_sub_bs:args.labeled_bs]\n",
    "            with torch.no_grad():\n",
    "\n",
    "                \n",
    "                pre_a = ema_model(uimg_a) # pseudo label a \n",
    "                pre_b = ema_model(uimg_b) # pseudo label b \n",
    "                plab_a = get_ACDC_masks(pre_a, nms=1)\n",
    "                plab_b = get_ACDC_masks(pre_b, nms=1)\n",
    "                img_mask, loss_mask = generate_mask(img_a)\n",
    "                unl_label = ulab_a * img_mask + lab_a * (1 - img_mask)\n",
    "                l_label = lab_b * img_mask + ulab_b * (1 - img_mask)\n",
    "            consistency_weight = get_current_consistency_weight(iter_num//150,args)\n",
    "\n",
    "            net_input_unl = uimg_a * img_mask + img_a * (1 - img_mask)\n",
    "            net_input_l = img_b * img_mask + uimg_b * (1 - img_mask)\n",
    "            out_unl = model.forward_segmentation(net_input_unl)\n",
    "            out_l = model.forward_reconstruction(net_input_l)\n",
    "\n",
    "            # forward mae\n",
    "            rec_out = model.forward_reconstruction(masked_img.cuda())\n",
    "\n",
    "\n",
    "            rec_loss = reconstruction_loss(rec_out, mae_batch, mask_up)\n",
    "\n",
    "\n",
    "            unl_dice, unl_ce = mix_loss(out_unl, plab_a, lab_a, loss_mask, u_weight=args.u_weight, unlab=True)\n",
    "            l_dice, l_ce = mix_loss(out_l, lab_b, plab_b, loss_mask, u_weight=args.u_weight)\n",
    "\n",
    "\n",
    "            loss_ce = unl_ce + l_ce \n",
    "            loss_dice = unl_dice + l_dice\n",
    "\n",
    "            total_loss = 1 * (loss_dice + loss_ce) + 1 * rec_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iter_num += 1\n",
    "            update_model_ema(model, ema_model, 0.99)\n",
    "\n",
    "            writer.add_scalar('info/total_loss', total_loss, iter_num)\n",
    "            writer.add_scalar('info/mix_dice', loss_dice, iter_num)\n",
    "            writer.add_scalar('info/mix_ce', loss_ce, iter_num)\n",
    "            writer.add_scalar('info/consistency_weight', consistency_weight, iter_num)     \n",
    "\n",
    "            logging.info('iteration %d: loss: %f, mix_dice: %f, mix_ce: %f'%(iter_num, total_loss, loss_dice, loss_ce))\n",
    "                \n",
    "            if iter_num % 20 == 0:\n",
    "                image = net_input_unl[1, 0:1, :, :]\n",
    "                writer.add_image('train/Un_Image', image, iter_num)\n",
    "                outputs = torch.argmax(torch.softmax(out_unl, dim=1), dim=1, keepdim=True)\n",
    "                writer.add_image('train/Un_Prediction', outputs[1, ...] * 50, iter_num)\n",
    "                labs = unl_label[1, ...].unsqueeze(0) * 50\n",
    "                writer.add_image('train/Un_GroundTruth', labs, iter_num)\n",
    "\n",
    "                image_l = net_input_l[1, 0:1, :, :]\n",
    "                writer.add_image('train/L_Image', image_l, iter_num)\n",
    "                outputs_l = torch.argmax(torch.softmax(out_l, dim=1), dim=1, keepdim=True)\n",
    "                writer.add_image('train/L_Prediction', outputs_l[1, ...] * 50, iter_num)\n",
    "                labs_l = l_label[1, ...].unsqueeze(0) * 50\n",
    "                writer.add_image('train/L_GroundTruth', labs_l, iter_num)\n",
    "\n",
    "            if iter_num > 0 and iter_num % 200 == 0:\n",
    "                model.eval()\n",
    "                metric_list = 0.0\n",
    "                for _, sampled_batch in enumerate(valloader):\n",
    "                    metric_i = test_single_volume(sampled_batch[\"image\"], sampled_batch[\"label\"], model, classes= num_classes)\n",
    "                    metric_list += np.array(metric_i)\n",
    "                metric_list = metric_list / len(db_val)\n",
    "                print(f'Metric list: {metric_list}') \n",
    "                for class_i in range(num_classes-1):\n",
    "                    writer.add_scalar('info/val_{}_dice'.format(class_i+1), metric_list[class_i, 0], iter_num)\n",
    "                    writer.add_scalar('info/val_{}_hd95'.format(class_i+1), metric_list[class_i, 1], iter_num)\n",
    "\n",
    "                performance = np.mean(metric_list, axis=0)[0]\n",
    "                writer.add_scalar('info/val_mean_dice', performance, iter_num)\n",
    "\n",
    "                if performance > best_performance:\n",
    "                    best_performance = performance\n",
    "                    save_mode_path = os.path.join(snapshot_path, 'iter_{}_dice_{}.pth'.format(iter_num, round(best_performance, 4)))\n",
    "                    save_best_path = os.path.join(snapshot_path,'{}_best_model.pth'.format(args.model))\n",
    "                    torch.save(model.state_dict(), save_mode_path)\n",
    "                    torch.save(model.state_dict(), save_best_path)\n",
    "\n",
    "                logging.info('iteration %d : mean_dice : %f' % (iter_num, performance))\n",
    "                model.train()\n",
    "\n",
    "            if iter_num >= max_iterations:\n",
    "                break\n",
    "        if iter_num >= max_iterations:\n",
    "            iterator.close()\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c4acdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain log path: ./model/BCP/ACDC_MBCP_7_labeled/pretrain/log.txt\n",
      "Self-train log path: ./model/BCP/ACDC_MBCP_7_labeled/selftrain/log.txt\n",
      "Mode: train_lab: 136 samples in total\n",
      "Mode: val: 20 samples in total\n",
      "Total slice is 1360, Labeled slice is 136\n",
      "Start pre-training\n",
      "11 iterations per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                           | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1: loss 2.307791, mix_dice: 1.661705, mix_ce: 2.953876\n",
      "iteration 2: loss 2.133703, mix_dice: 1.631261, mix_ce: 2.636145\n",
      "iteration 3: loss 2.011671, mix_dice: 1.605264, mix_ce: 2.418077\n",
      "iteration 4: loss 1.911551, mix_dice: 1.571022, mix_ce: 2.252081\n",
      "iteration 5: loss 1.813652, mix_dice: 1.529119, mix_ce: 2.098186\n",
      "iteration 6: loss 1.758866, mix_dice: 1.536966, mix_ce: 1.980766\n",
      "iteration 7: loss 1.676482, mix_dice: 1.485111, mix_ce: 1.867853\n",
      "iteration 8: loss 1.616066, mix_dice: 1.493103, mix_ce: 1.739028\n",
      "iteration 9: loss 1.539205, mix_dice: 1.463070, mix_ce: 1.615341\n",
      "iteration 10: loss 1.513194, mix_dice: 1.513221, mix_ce: 1.513168\n",
      "iteration 11: loss 1.416934, mix_dice: 1.404958, mix_ce: 1.428910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████                            | 1/5 [00:01<00:07,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12: loss 1.369832, mix_dice: 1.420684, mix_ce: 1.318980\n",
      "iteration 13: loss 1.312752, mix_dice: 1.445195, mix_ce: 1.180309\n",
      "iteration 14: loss 1.262157, mix_dice: 1.406788, mix_ce: 1.117525\n",
      "iteration 15: loss 1.234733, mix_dice: 1.405270, mix_ce: 1.064196\n",
      "iteration 16: loss 1.172330, mix_dice: 1.400735, mix_ce: 0.943925\n",
      "iteration 17: loss 1.126444, mix_dice: 1.357634, mix_ce: 0.895254\n",
      "iteration 18: loss 1.086958, mix_dice: 1.335052, mix_ce: 0.838865\n",
      "iteration 19: loss 1.048236, mix_dice: 1.328603, mix_ce: 0.767869\n",
      "iteration 20: loss 1.045889, mix_dice: 1.365126, mix_ce: 0.726652\n",
      "iteration 21: loss 0.996591, mix_dice: 1.338143, mix_ce: 0.655039\n",
      "iteration 22: loss 1.006549, mix_dice: 1.369876, mix_ce: 0.643222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████                     | 2/5 [00:03<00:04,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 23: loss 0.999990, mix_dice: 1.338724, mix_ce: 0.661256\n",
      "iteration 24: loss 0.978765, mix_dice: 1.353532, mix_ce: 0.603999\n",
      "iteration 25: loss 0.932210, mix_dice: 1.301591, mix_ce: 0.562829\n",
      "iteration 26: loss 0.929617, mix_dice: 1.353225, mix_ce: 0.506009\n",
      "iteration 27: loss 0.946498, mix_dice: 1.411516, mix_ce: 0.481480\n",
      "iteration 28: loss 0.944127, mix_dice: 1.373029, mix_ce: 0.515224\n",
      "iteration 29: loss 0.847859, mix_dice: 1.255918, mix_ce: 0.439801\n",
      "iteration 30: loss 0.846381, mix_dice: 1.265321, mix_ce: 0.427440\n",
      "iteration 31: loss 0.866975, mix_dice: 1.287335, mix_ce: 0.446616\n",
      "iteration 32: loss 0.894379, mix_dice: 1.293718, mix_ce: 0.495039\n",
      "iteration 33: loss 0.970430, mix_dice: 1.369877, mix_ce: 0.570984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████              | 3/5 [00:04<00:02,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 34: loss 0.898077, mix_dice: 1.328545, mix_ce: 0.467609\n",
      "iteration 35: loss 0.846893, mix_dice: 1.282130, mix_ce: 0.411655\n",
      "iteration 36: loss 0.801106, mix_dice: 1.216877, mix_ce: 0.385334\n",
      "iteration 37: loss 0.898235, mix_dice: 1.358977, mix_ce: 0.437493\n",
      "iteration 38: loss 0.872980, mix_dice: 1.358505, mix_ce: 0.387455\n",
      "iteration 39: loss 0.905890, mix_dice: 1.342122, mix_ce: 0.469658\n",
      "iteration 40: loss 0.896961, mix_dice: 1.345330, mix_ce: 0.448591\n",
      "iteration 41: loss 0.809853, mix_dice: 1.225058, mix_ce: 0.394649\n",
      "iteration 42: loss 0.851451, mix_dice: 1.294019, mix_ce: 0.408883\n",
      "iteration 43: loss 0.940976, mix_dice: 1.464186, mix_ce: 0.417766\n",
      "iteration 44: loss 0.784245, mix_dice: 1.237287, mix_ce: 0.331203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████       | 4/5 [00:06<00:01,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 45: loss 0.902968, mix_dice: 1.388827, mix_ce: 0.417108\n",
      "iteration 46: loss 0.916643, mix_dice: 1.379431, mix_ce: 0.453854\n",
      "iteration 47: loss 0.780852, mix_dice: 1.219011, mix_ce: 0.342694\n",
      "iteration 48: loss 0.841862, mix_dice: 1.359953, mix_ce: 0.323772\n",
      "iteration 49: loss 0.753177, mix_dice: 1.238876, mix_ce: 0.267478\n",
      "iteration 50: loss 0.881429, mix_dice: 1.320369, mix_ce: 0.442489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████       | 4/5 [00:07<00:01,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: train_lab: 136 samples in total\n",
      "Mode: reconstruct: 1902 samples in total\n",
      "Mode: val: 20 samples in total\n",
      "Total slices is: 1360, labeled slices is:136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from ./model/BCP/ACDC_MBCP_7_labeled/pretrain/unet_best_model.pth\n",
      "Loaded from ./model/BCP/ACDC_MBCP_7_labeled/pretrain/unet_best_model.pth\n",
      "Start self_training\n",
      "Start self_training\n",
      "11 iterations per epoch\n",
      "11 iterations per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                           | 0/1 [00:00<?, ?it/s]/tmp/ipykernel_19622/2815246112.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  return torch.Tensor(batch_list).cuda()\n",
      "  0%|                                           | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 3.80 GiB of which 175.69 MiB is free. Including non-PyTorch memory, this process has 3.58 GiB memory in use. Of the allocated memory 3.44 GiB is allocated by PyTorch, and 37.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(filename\u001b[38;5;241m=\u001b[39mself_snapshot_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/log.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%(msecs)03d\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, datefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger()\u001b[38;5;241m.\u001b[39maddHandler(logging\u001b[38;5;241m.\u001b[39mStreamHandler(sys\u001b[38;5;241m.\u001b[39mstdout))\n\u001b[0;32m---> 28\u001b[0m \u001b[43mself_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_snapshot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_snapshot_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 92\u001b[0m, in \u001b[0;36mself_train\u001b[0;34m(args, pre_snapshot_path, snapshot_path)\u001b[0m\n\u001b[1;32m     89\u001b[0m out_l \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward_reconstruction(net_input_l)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# forward mae\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m rec_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_reconstruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m rec_loss \u001b[38;5;241m=\u001b[39m reconstruction_loss(rec_out, mae_batch, mask_up)\n\u001b[1;32m     98\u001b[0m unl_dice, unl_ce \u001b[38;5;241m=\u001b[39m mix_loss(out_unl, plab_a, lab_a, loss_mask, u_weight\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mu_weight, unlab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[18], line 32\u001b[0m, in \u001b[0;36mMBCP.forward_reconstruction\u001b[0;34m(self, x_masked)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_reconstruction\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_masked):\n\u001b[1;32m     31\u001b[0m     feat_masked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x_masked)\n\u001b[0;32m---> 32\u001b[0m     rec_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_masked\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Reconstruction output [B, in_chns, H, W]\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rec_out\n",
      "File \u001b[0;32m~/miniconda3/envs/code/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/code/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 131\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, feature, apply_grn)\u001b[0m\n\u001b[1;32m    129\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(x, x2, apply_grn)\n\u001b[1;32m    130\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(x, x1, apply_grn)\n\u001b[0;32m--> 131\u001b[0m x_last \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_conv(x_last)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, x_last\n",
      "File \u001b[0;32m~/miniconda3/envs/code/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/code/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 77\u001b[0m, in \u001b[0;36mUpBlock.forward\u001b[0;34m(self, x1, x2, apply_grn)\u001b[0m\n\u001b[1;32m     75\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvx1(x1)\n\u001b[1;32m     76\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup(x1)\n\u001b[0;32m---> 77\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x, apply_grn\u001b[38;5;241m=\u001b[39mapply_grn)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 3.80 GiB of which 175.69 MiB is free. Including non-PyTorch memory, this process has 3.58 GiB memory in use. Of the allocated memory 3.44 GiB is allocated by PyTorch, and 37.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Trainig process \n",
    "if args.deterministic: \n",
    "    cudnn.benchmark = False \n",
    "    cudnn.deterministic = True \n",
    "    random.seed(args.seed) \n",
    "    np.random.seed(args.seed) \n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Path \n",
    "pre_snapshot_path = \"./model/BCP/ACDC_{}_{}_labeled/pretrain\".format(args.exp, args.label_num)\n",
    "self_snapshot_path = \"./model/BCP/ACDC_{}_{}_labeled/selftrain\".format(args.exp, args.label_num)\n",
    "\n",
    "print(f'Pretrain log path: {pre_snapshot_path + \"/log.txt\"}')\n",
    "print(f'Self-train log path: {self_snapshot_path + \"/log.txt\"}')\n",
    "\n",
    "for snapshot_path in [pre_snapshot_path, self_snapshot_path]: \n",
    "    if not os.path.exists(snapshot_path): \n",
    "        os.makedirs(snapshot_path, exist_ok= True)\n",
    "#Pre_train\n",
    "logging.basicConfig(filename=pre_snapshot_path+\"/log.txt\", level=logging.INFO, format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "pre_train(args, pre_snapshot_path)\n",
    "\n",
    "#Self_train\n",
    "logging.basicConfig(filename=self_snapshot_path+\"/log.txt\", level=logging.INFO, format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
    "logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "self_train(args, pre_snapshot_path, self_snapshot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11e32b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
